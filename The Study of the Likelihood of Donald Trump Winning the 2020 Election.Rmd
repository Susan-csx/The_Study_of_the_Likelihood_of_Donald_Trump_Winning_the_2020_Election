---
title: "The Study of the Likelihood of Donald Trump Winning the 2020 Election"
author: "Shuxian Cao, Xueying Fu, Yichen Su, Yiyang Huang"
date: "28 October 2020"
output:
  bookdown::pdf_document2
notice: "@*"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(dplyr)
library(ggplot2)
library(knitr)
library(gridExtra)

# Loading in the cleaned survey Data
survey_data <- read_csv("survey_data.csv")
nona_survey <- na.omit(survey_data)

# Loading in the cleaned census Data
census_data <- read_csv("census_data.csv")
```

# Information

**Topic:** The Study of the Likelihood of Donald Trump Winning the 2020 Election

**Author:** Shuxian Cao, Xueying Fu, Yichen Su, Yiyang Huang

**Date:** October 28, 2020

**Code and data supporting this analysis is available at:**

https://github.com/Susan-csx/The_Study_of_the_Likelihood_of_Donald_Trump_Winning_the_2020_Election.git

# Model

## Model Selection

To estimate Donald Trump's chances of winning the 2020 American federal election, we choose to use the Multiple Logistic Regression Model with post-stratification. In comparison to Multiple Linear Regression, a Multiple Logistic Regression can be used to predict a discrete binary outcome based on several independent variables, while a linear regression is better at giving numeric predictions. In our study, the response variable `vote_trump` is a binary variable that takes the value 1 if the respondent claimed to vote for Trump and 0 otherwise. Thus, fitting a Multiple Logistic Regression Model to our data is the best choice available for us. Then, post-stratification allows us to divide all observations into different cells, which contributes to a better prediction. The whole process can be divided into three major steps: data cleaning, modeling, and post-stratification.

## Data Selection

The survey data that we are dealing with is a raw data set, so performing a data cleaning is very necessary. First, we removed all observations with NA in our survey data because missing values may affect the accuracy of predictions. To use the post-stratification, we performed some mutations on variables to make sure the variables in both survey data and census data matches. The marital status (`marst`) is characterized into 1 and 0, where 1 means married and 0 means unmarried. `census_region` in survey data can be matched to `region` in census data, so we grouped `region` to "Northeast", "Midwest", "South", and "West", which matches the subcategories in `census_region`. For the same reason, `race` was grouped according to subcategories in `race_ethnicity`, and people aged above 17 and under 94 were selected.

```{r, include = FALSE}
# Rename gender variable to sex in nona_survey data
unique(nona_survey$gender)
unique(census_data$sex)
nona_survey <- nona_survey %>%
  mutate(sex = if_else(gender == "Female", "female", "male"))

# Rename employment variable to labforce in nona_survey data
unique(nona_survey$employment)
unique(census_data$labforce)
nona_survey <- nona_survey %>%
  mutate(labforce = if_else(employment == "Full-time employed", "yes, in the labor force",
                    if_else(employment == "Unemployed or temporarily on layoff", "no, not in the labor force",
                    if_else(employment == "Retired", "no, not in the labor force",
                    if_else(employment == "Student", "no, not in the labor force",
                    if_else(employment == "Homemaker", "no, not in the labor force",
                    if_else(employment == "Part-time employed", "yes, in the labor force", 
                    if_else(employment == "Self-employed", "yes, in the labor force", 
                    if_else(employment == "Permanently disabled", "no, not in the labor force", "no, not in the labor force")))))))))

# Rename race_ethnicity variable to race in nona_survey data
unique(nona_survey$race_ethnicity)
unique(census_data$race)
nona_survey <- nona_survey %>%
  mutate(race = if_else(race_ethnicity == "White", "white",
                if_else(race_ethnicity == "Black, or African American", "black/african american/negro",
                if_else(race_ethnicity == "Asian (Asian Indian)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Vietnamese)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Chinese)", "chinese",
                if_else(race_ethnicity == "Asian (Korean)", "other asian or pacific islander", 
                if_else(race_ethnicity == "Asian (Japanese)", "japanese", 
                if_else(race_ethnicity == "Asian (Filipino)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Other)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Native Hawaiian)", "other asian or pacific islander",
                if_else(race_ethnicity == "American Indian or Alaska Native", "american indian or alaska native",
                if_else(race_ethnicity == "Pacific Islander (Other)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Samoan)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Guamanian)", "other asian or pacific islander", "other race, nec")))))))))))))))

# Remove the columns that have been changed/mutated (Deleted vote_2016, vote_2020, ideo5, household_income)
nona_survey <- nona_survey %>%
  select(interest, registration, vote_intention,
         labforce, foreign_born, sex, census_region,
         hispanic, race, education, state, congress_district, age, vote_trump)

```

## Model Specifics

We chose to perform a Multiple Logistic Regression on our data to model the likelihood of Donald Trump winning the 2020 Election because our response variable is binary. Also, a Multiple Logistic Regression "doesn't require input features to be scaled" and the result can be easily interpreted and understood (Donges, 2018). We started by removing `vote_2016`, `ideo5`, and `household_income` because they were not available in our census data. The model selection was done by comparing the AIC values, which measured if a model "balances the goodness of fit with simplicity" (Chen, Hu & Yang, n.d.). A lower AIC value suggests a better-fit model. 

We started with fitting all the remaining variables with a multiple logistic model, which gave an AIC value of 7447. Then, `congress_distrist`, `vote_intention`, `registration`, and `foreign_born ` were removed from the model, and the new model gives an AIC value of 7206, which means the reduced model is better than the full model. By repeating this process, our final model with variables `age`, `labforce`, `sex`, `census_region`, and `race` gave an AIC value of 7203. According to Figure \@ref(fig:fig2), we can tell people aged under 30 are less likely to vote for Donald Trump in comparison to those aged above 30. Also, considering Donald Trump's comments about women, how male and female favor Trump may also differ, and this is proved by the first plot in Figure \@ref(fig:fig1). Thus, it is not surprising that these variables are included in our final model.(All the AIC values are shown in \@ref(tab:table2).)

The model is run by R, and here is how our model looks like:

$$
log(\frac{p}{1-p}) = \beta_0 + \beta_1 X_{1age} + \beta_2 X_{2Labor_{Yes}} + \beta_3 X_{3Male} + \beta_4 X_{4Region_{NW}} + \beta_5 X_{5Region_{S}} + \beta_6 X_{6Region_{W}}
$$
$$
+ \beta_7 X_{7Race_{B.AA.N}} + \beta_8 X_{8Race_{CN}} + \beta_9 X_{9Race_{JP}} + \beta_{10} X_{10Race_{O.Asian}} + \beta_{11} X_{11Race_{Other}} + \beta_{12} X_{12Race_{White}} + \epsilon,
$$

where:

- $log$ represents the natural logarithm

- $p$ represents the probability of Trump winning the 2020 Election

- $\frac{1}{1-p}$ represents the "odd ratio"

- $log(\frac{p}{1-p})$ represents the log odds ratio

- $X_i$ represents the predictor variables in our model

- $X_{1age}$ represents the input value of age

- For $X_i$ from $X_2$ to $X_{12}$, $X_i$ are dummy variables and each of them takes the value 0 or 1

- $\beta_0$ is the constant term which represents the intercept at time zero. It is the value of $logit(p)$ when every $X_i = 0$

- $\beta_i$s are the coefficients for each of the subcategories which represent the average difference in the log of odds ratio between $X_i = 0$ and $X_i = 1$ holding other variables constant 

(e.g. $\beta_3$ represents the average difference in the log of odds ratio between male and female holding other variables constant)

- $\epsilon$ represents the error term

```{r, include = FALSE}
# Step 1: With all variables
# Creating the Model (AIC 7446.8)
model_with_full <- glm(vote_trump ~ age + registration + interest + vote_intention + labforce + foreign_born + sex + census_region + hispanic + race + education + state + congress_district, data = nona_survey, family =  "binomial")

# Step2: Without congress_distrist, vote_intention, registration, foreign_born (AIC 7206.4)
model1 <- glm(vote_trump ~ age + interest + labforce + sex + census_region + hispanic + race + education + state, data = nona_survey, family =  "binomial")

# Step3: 1 Without interest, hispanic, state, education (AIC 7202.8)
survey_model <- glm(vote_trump ~ age + labforce + sex + census_region + race, data = nona_survey, family =  "binomial")

```

## Post-Stratification 

To provide a better prediction on Trump's chances of winning the election, we performed a post-stratification analysis. It "adjusts the sampling and replicate weights so that the joint distribution of a set of post-stratifying variables matches the known population joint distribution"(Lumley, n.d.). Since we had done the data cleaning before, our data was prepared to do the post-stratification at this time. We started by partitioning the data into demographic cells based on each subcategory of each predictor variable. We decided to divide all of them because we predicted there were differences among these subcategories (based on Figure \@ref(fig:fig1), which was plot using the survey data). The plot will be discussed in the discussion section of this report. Then, we used the model above to estimate Trump's chances of winning the election for each cell. The final step was to do the calculations based on the formula:
$$
\hat{y}^{PS} = \frac{\sum{N_j{\hat{y}_j}}}{\sum{N_j}}
$$

Where $\hat{y}^{PS}$ represents the likelihood of Donald Trump winning the 2020 Election
$\hat{y}_j$ hat represents the estimated likelihood in $j^{th}$ cell
$N_j$ represents the population size in $j^{th}$ cells
$\sum{N_j}$ represents the entire population


\newpage

# Results

```{r table1}
# Here I will perform the post-stratification calculation
census_data$logodds_estimate <-
  survey_model %>%
  predict(newdata = census_data)

census_data$estimate <-
  exp(census_data$logodds_estimate)/(1+exp(census_data$logodds_estimate))

post_strat_predict <- census_data %>%
  mutate(alp_predict_prop = estimate*n) %>%
  summarise(predict = sum(alp_predict_prop)/sum(n))

kable(post_strat_predict, format = "markdown", digits = 3, align = "c", 
      caption = "The Predicted Proportion of Voters for Republican Party")
```

The proportion of voters for the Republican Party (The candidate of this party is Donald Trump.), with an equation of ${\hat{y}}^{PS} = \frac{\sum{N_j{\hat{y}_j}}}{\sum{N_j}}$, is estimated to be **0.409**, meaning that there are less than a half voters id going to vote for Donald Trump. This prediction is based on our post-stratification analysis of the proportion of voters in favor of the Republican Party models by a logistic regression model, `survey_model`, which accounted for `age`, `labforce` (i.e employment status), `sex`, `census region`, and `race`.

```{r summary aic, include = FALSE}
summary(model_with_full)
summary(model1)
summary(survey_model)
```

```{r table2}
aic_summary <- data.frame(Model_Name = c("model_with_full", "model1", "surbey_model"), 
            AIC = c(7446.8, 7206.4,7202.8))
kable(aic_summary, align = "c", format = "markdown", col.names = c("Model Name", "AIC"),
      caption = "Summary - AIC Score of Each Potential Model")
```

By summarizing the three potential models in Part I, we formed an AIC table here. The AIC of`model_with_full`, which includes all the variable in the cleaned survey dataset, `nona_survey`, is equal to 7446.8, while the one of `model1` decreases to 7206.4, and the lowest AIC appears in our final logistic regression model called `survey_model`.

```{r table3}
kable(broom::tidy(survey_model), align = "c", format = "markdown", digits = 3,
      col.names = c("Term", "Estimate", "Standard Error", "Test Statistic", "P-Value"),
      caption = "Summary for the Survey Logistic Regression Model")
```

Since `survey_model` is the final model we chose for predict the proportion of potential voters for Donald Trump, we also formed a summary table with the estimated values of the intercept, $\hat{\beta_0}$ and estimated slopes $\hat{\beta_i}$ (for i = 1,...,12) as well as standard errors, test statistics and p-value. Therefore, the fitted logistic model is:
$$
log(\frac{\hat{p}}{1-\hat{p}}) = -1.529 + 0.016 X_{age} + 0.287 X_{Labor_{Yes}} + 0.422 X_{Male} -0.119 X_{Region_{NW}} + 0.294 X_{Region_{S}} - 0.081 X_{Region_{W}}
$$

$$
- 1.87 X_{Race_{B.AA.N}} - 1.167 X_{Race_{CN}} - 0.603 X_{Race_{JP}} - 0.459 X_{Race_{O.Asian}} - 0.604 X_{Race_{Other}} + 0.14 X_{Race_{White}}.
$$

\newpage

```{r fig1, fig.cap = "Relationship between Voters' Choice and Number of Voters by Sex, Employment Status, Census Region and Race"}
hist_sex <- ggplot(nona_survey, aes(as.character(vote_trump), fill = sex)) + 
  geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Sex") +
  theme(plot.title = element_text(hjust = 0))

hist_lab <- ggplot(nona_survey, aes(as.character(vote_trump), fill = labforce)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People",
       fill = "Employment Status")

hist_region <- ggplot(nona_survey, aes(as.character(vote_trump), fill = census_region)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Census Region")

hist_race <- ggplot(nona_survey, aes(as.character(vote_trump), fill = race)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Race")

grid.arrange(hist_sex, hist_lab, hist_region, hist_race, nrow = 2, widths = c(1.55, 2), newpage = FALSE)
```

Four histograms above illustrate the relationships between the voters' choice  and number of voters in the 2020 election, grouped by some of the independent variables in the logistic `survey_model`, `sex`, `labforce`, `census_region` and `race`. 

```{r fig2, fig.cap = "Trump's Potential Voters, Depending on Sex and Age"}
jitter <- nona_survey %>%
  ggplot(aes(as.character(vote_trump), age)) +  
  facet_grid(.~sex) +
  geom_jitter(aes(color = age),alpha = 0.4) +
  theme_bw() +
  labs(x = "Whether vote for Donald Trump", y = "Age") 

grid.arrange(jitter, newpage = FALSE)
```

After grouping the data points by sex, the above scatterplot enables us to observe how both of `sex` and `age` of the voters affect the choice they make towards the 2020 election in the United States.

# Discussion

From the result, we predict that only 41 % of the voters will vote for Donald Trump from the Republican Party, which has no more than half of the people voting for him. And this might relate to some facts we observe from figure  \@ref(fig:fig1) and  \@ref(fig:fig2). We can see a large proportion of women do not want to vote for Donald Trump, especially for ages between 20 to 30. This can relate to his sexism, disrespected behavior, and language toward women in the past. It reflects that the odds for males are 52% higher than the odds of females voting for Donald Trump. Also, there is a large proportion of black and other races that do not want to vote for Trump. This can be due to the racist wordings and behavior of many races. And the recent case of “George Floyd’s death” also gains the black's hatred of Donald Trump, which also raises the discrimination issues of disproportioned deaths on African-Americans. It shows that the odds for black voting for Trump are 84% lower than the odds for other races. Therefore, the predicted proportion of voting for Donald Trump is less than half can be highly related to his inappropriate behaviors in the past or even the rise of racism and inequality in America.

## Weaknesses

Here we discuss weaknesses of the study, data, analysis, etc. You can also discuss areas for improvement.

## Next Steps

Here you discuss subsequent work to be done after this report. This can include next steps in terms of statistical analysis (perhaps there is a more efficient algorithm available, or perhaps there is a caveat in the data that would allow for some new technique). Future steps should also be specified in terms of the study setting (eg. including a follow-up survey on something, or a subsequent study that would complement the conclusions of your report).


# References

## Survey Data and Census Data

Steven Ruggles, Sarah Flood, Ronald Goeken, Josiah Grover, Erin Meyer, Jose Pacas and Matthew Sobek. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IPUMS USA: Version 10.0 [dataset]. Minneapolis, MN: IPUMS, 2020. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://doi.org/10.18128/D010.V10.0

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (version 20200814). Retrieved from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://www.voterstudygroup.org/downloads?key=44f0708b-e919-44bb-9695-2846c3531bd5.

## Other References

Auguie,B (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics. R package version 2.3.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://CRAN.R-project.org/package=gridExtra

Chen,H.W., Hu,X. & Yang,Z.C. (n.d.). Model Selection for Linear Regression Model. Retrieved from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  https://jbhender.github.io/Stats506/F17/Projects/Group21_Model_Selection.html

Donges,N. (2018). The Logistic Regression: Algorithm. Retrieved from

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://machinelearning-blog.com/2018/04/23/logistic-regression-101/

Lopez,G. (2016). Donald Trump's long history of racism, from the 1970s to 2020. Retrieved from: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://www.vox.com/2016/7/25/12270880/donald-trump-racist-racism-history.

Lumley,T. (n.d.). postStratify. Retrieved from https://www.rdocumentation.org/packages/survey/

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; versions/2.4/topics/postStratify

Reston,M. (2020). With a shocking invocation of George Floyd, Trump shows his disconnect from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nation's pain. Retrieved from https://www.cnn.com/2020/06/06/politics/trump-george-floyd-maine/

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; index.html.

Wickham,H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.

Wickham,H., François,R., Henry,L. & Müller, K. (2020). dplyr: A Grammar of Data 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Manipulation. R package version 1.0.2. https://CRAN.R-project.org/package=dplyr

Wolffe,R. (2016). Donald Trump's woman problem: they don't like him, not one little bit | Richard 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wolffe. Retrieved from: https://www.theguardian.com/commentisfree/2016/jun/03/donald-trump-

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; woman-problem-female-voter-support-poll-numbers.

Wickham,H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://doi.org/10.21105/joss.01686
  
Xie,Y. (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Chapman and Hall/CRC. ISBN 978-1466561595
  
Xie,Y. (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ISBN 978-1498716963

Xie,Y. (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R package version 1.30.
