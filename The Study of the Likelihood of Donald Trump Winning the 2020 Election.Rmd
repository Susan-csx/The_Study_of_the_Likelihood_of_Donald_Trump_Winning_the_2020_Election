---
title: "The Study of the Likelihood of Donald Trump Winning the 2020 Election"
author: "Shuxian Cao, Xueying Fu, Yichen Su, Yiyang Huang"
date: "28 October 2020"
output:
  bookdown::pdf_document2
notice: "@*"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(dplyr)
library(ggplot2)
library(knitr)
library(gridExtra)

# Loading in the cleaned survey Data
survey_data <- read_csv("survey_data.csv")
nona_survey <- na.omit(survey_data)

# Loading in the cleaned census Data
census_data <- read_csv("census_data.csv")
```

# Information

**Topic:** The Study of the Likelihood of Donald Trump Winning the 2020 Election

**Author:** Shuxian Cao, Xueying Fu, Yichen Su, Yiyang Huang

**Date:** October 28, 2020

**Code and data supporting this analysis is available at:**

https://github.com/Susan-csx/The_Study_of_the_Likelihood_of_Donald_Trump_Winning_the_2020_Election.git

# Model

## Model Selection
To estimate Donald Trump's chances of winning the 2020 American federal election, we chose to use the Multiple Logistic Regression Model with post-stratification. In comparison to Multiple Linear Regression, a Multiple Logistic Regression can be used to predict a discrete binary outcome based on several independent variables, while a linear regression is better at giving numeric predictions. In our study, the response variable `vote_trump` is a binary variable that takes the value 1 if the respondent claimed to vote for Trump and 0 otherwise. Thus, fitting a Multiple Logistic Regression Model to our data is the best choice available for us. Then, post-stratification allows us to divide all observations into different cells, which contributes to a better prediction. The whole process can be divided into three major steps: data cleaning, modeling, and post-stratification.

## Data Cleaning
The survey data that we are dealing with is a raw data set, so performing a data cleaning is very necessary. First, we removed all observations with NA in our survey data because missing values may affect the accuracy of predictions. To use the post-stratification, we performed some mutations on variables to make sure the variables in both survey data and census data matches. The marital status (`marst`) is characterized into 1 and 0, where 1 means married and 0 otherwise. `census_region` in survey data can be matched to `region` in census data, so we grouped `region` to "Northeast", "Midwest", "South", and "West", which matches the subcategories in `census_region`. For the same reason, `race` was grouped according to subcategories in `race_ethnicity`, and people aged above 17 and under 94 were selected.

```{r, include = FALSE}
# Rename gender variable to sex in nona_survey data
unique(nona_survey$gender)
unique(census_data$sex)
nona_survey <- nona_survey %>%
  mutate(sex = if_else(gender == "Female", "female", "male"))

# Rename employment variable to labforce in nona_survey data
unique(nona_survey$employment)
unique(census_data$labforce)
nona_survey <- nona_survey %>%
  mutate(labforce = if_else(employment == "Full-time employed", "yes, in the labor force",
                    if_else(employment == "Unemployed or temporarily on layoff", "no, not in the labor force",
                    if_else(employment == "Retired", "no, not in the labor force",
                    if_else(employment == "Student", "no, not in the labor force",
                    if_else(employment == "Homemaker", "no, not in the labor force",
                    if_else(employment == "Part-time employed", "yes, in the labor force", 
                    if_else(employment == "Self-employed", "yes, in the labor force", 
                    if_else(employment == "Permanently disabled", "no, not in the labor force", "no, not in the labor force")))))))))

# Rename race_ethnicity variable to race in nona_survey data
unique(nona_survey$race_ethnicity)
unique(census_data$race)
nona_survey <- nona_survey %>%
  mutate(race = if_else(race_ethnicity == "White", "white",
                if_else(race_ethnicity == "Black, or African American", "black/african american/negro",
                if_else(race_ethnicity == "Asian (Asian Indian)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Vietnamese)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Chinese)", "chinese",
                if_else(race_ethnicity == "Asian (Korean)", "other asian or pacific islander", 
                if_else(race_ethnicity == "Asian (Japanese)", "japanese", 
                if_else(race_ethnicity == "Asian (Filipino)", "other asian or pacific islander",
                if_else(race_ethnicity == "Asian (Other)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Native Hawaiian)", "other asian or pacific islander",
                if_else(race_ethnicity == "American Indian or Alaska Native", "american indian or alaska native",
                if_else(race_ethnicity == "Pacific Islander (Other)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Samoan)", "other asian or pacific islander",
                if_else(race_ethnicity == "Pacific Islander (Guamanian)", "other asian or pacific islander", "other race, nec")))))))))))))))

# Remove the columns that have been changed/mutated (Deleted vote_2016, vote_2020, ideo5, household_income)
nona_survey <- nona_survey %>%
  select(interest, registration, vote_intention,
         labforce, foreign_born, sex, census_region,
         hispanic, race, education, state, congress_district, age, vote_trump)

```
## Model Specifics
We chose to perform a Multiple Logistic Regression on our data to model the likelihood of Donald Trump winning the 2020 Election because our response variable is binary. Also, a Multiple Logistic Regression "doesn't require input features to be scaled" and the result can be easily interpreted and understood (Donges, 2018). We started by removing `vote_2016`, `ideo5`, and `household_income` because they were not available in our census data. The model selection was done by comparing the AIC values, which measured if a model "balances the goodness of fit with simplicity" (Chen, Hu & Yang, n.d.). A lower AIC value suggests a better-fit model. 

We started with fitting all the remaining variables with a multiple logistic model, which gave an AIC value of 7446.8. Then, `congress_distrist`, `vote_intention`, `registration`, and `foreign_born ` were removed from the model, and the new model gives an AIC value of 7206.4, which means the reduced model is better than the full model. By repeating this process, our final model with variables `age`, `labforce`, `sex`, `census_region`, and `race` gave an AIC value of 7202.8. According to Figure \@ref(fig:fig2), we can tell people aged under 30 are less likely to vote for Donald Trump in comparison to those aged above 30. Also, considering Donald Trump's comments about women, how male and female favor Trump may also differ, and this is proved by the first plot in Figure \@ref(fig:fig1). Thus, it is not surprising that these variables are included in our final model.(All the AIC values are shown in Table \@ref(tab:table2).) The model is run by R, and here is how our model looks like:
$$
log(\frac{p}{1-p}) = \beta_0 + \beta_1 X_{1age} + \beta_2 X_{2Labor_{Yes}} + \beta_3 X_{3Male} + \beta_4 X_{4Region_{NW}} + \beta_5 X_{5Region_{S}} + \beta_6 X_{6Region_{W}}
$$
$$
+ \beta_7 X_{7Race_{B.AA.N}} + \beta_8 X_{8Race_{CN}} + \beta_9 X_{9Race_{JP}} + \beta_{10} X_{10Race_{O.Asian}} + \beta_{11} X_{11Race_{Other}} + \beta_{12} X_{12Race_{White}} + \epsilon,
$$

where $log$, as the natural logarithm, $p$, as the probability of Trump winning the 2020 Election, and $\frac{1}{1-p}$ as the "odd ratio" form a notation: $log(\frac{p}{1-p})$, which represents the log odds ratio. $X_i$ in the equation above represents the predictor variables in our model. For example, $X_{1age}$ is the input value of age, while for those from $X_2$ to $X_{12}$, $X_i$ are dummy variables and each of them takes the value 0 or 1. The coefficients for each of the subcategories are $\beta_i$s (for i from 1 to 12), which represent the average difference in the log of odds ratio between $X_i = 0$ and $X_i = 1$ holding other variables constant (e.g. $\beta_3$ represents the average difference in the log of odds ratio between male and female holding other variables constant). $\beta_0$, on the other hand is the constant term which represents the intercept at time zero. It is the value of $logit(p)$ when every $X_i = 0$. At last, $\epsilon$ is the error term of this model.

```{r, include = FALSE}
# Step 1: With all variables
# Creating the Model (AIC 7446.8)
model_with_full <- glm(vote_trump ~ age + registration + interest + vote_intention + labforce + foreign_born + sex + census_region + hispanic + race + education + state + congress_district, data = nona_survey, family =  "binomial")

# Step2: Without congress_distrist, vote_intention, registration, foreign_born (AIC 7206.4)
model1 <- glm(vote_trump ~ age + interest + labforce + sex + census_region + hispanic + race + education + state, data = nona_survey, family =  "binomial")

# Step3: 1 Without interest, hispanic, state, education (AIC 7202.8)
survey_model <- glm(vote_trump ~ age + labforce + sex + census_region + race, data = nona_survey, family =  "binomial")

```
## Post-Stratification 
To provide a better prediction on Trump's chances of winning the election, we performed a post-stratification analysis. It "adjusts the sampling and replicate weights so that the joint distribution of a set of post-stratifying variables matches the known population joint distribution"(Lumley, n.d.). Since we had done the data cleaning before, our data was prepared to do the post-stratification at this time. We started by partitioning the data into demographic cells based on each subcategory of each predictor variable. We decided to divide all of them because we predicted there were differences among these subcategories (based on Figure \@ref(fig:fig1), which was plot using the survey data). The plot will be discussed in the discussion section of this report. Then, we used the model above to estimate Trump's chances of winning the election for each cell. The final step was to do the calculations based on the formula:
$$
\hat{y}^{PS} = \frac{\sum{N_j{\hat{y}_j}}}{\sum{N_j}},
$$

$\hat{y}^{PS}$ in the equation above is the likelihood of Donald Trump winning the 2020 Election, where $\hat{y}_j$ is the estimated likelihood in $j^{th}$ cell, $N_j$ is the population size in $j^{th}$ cells and $\sum{N_j}$ is the entire population.

# Results

```{r table1}
# Here I will perform the post-stratification calculation
census_data$logodds_estimate <-
  survey_model %>%
  predict(newdata = census_data)

census_data$estimate <-
  exp(census_data$logodds_estimate)/(1+exp(census_data$logodds_estimate))

post_strat_predict <- census_data %>%
  mutate(alp_predict_prop = estimate*n) %>%
  summarise(predict = sum(alp_predict_prop)/sum(n))

kable(post_strat_predict, format = "markdown", digits = 3, align = "c", 
      caption = "The Predicted Proportion of Voters for Republican Party")
```

The proportion of voters for the Republican Party (The candidate of this party is Donald Trump.), with an equation of ${\hat{y}}^{PS} = \frac{\sum{N_j{\hat{y}_j}}}{\sum{N_j}}$, is estimated to be **0.409**, meaning that there are less than a half voters id going to vote for Donald Trump. This prediction is based on our post-stratification analysis of the proportion of voters in favor of the Republican Party models by a logistic regression model, `survey_model`, which accounted for `age`, `labforce` (i.e employment status), `sex`, `census region`, and `race`.

```{r summary aic, include = FALSE}
summary(model_with_full)
summary(model1)
summary(survey_model)
```

```{r table2}
aic_summary <- data.frame(Model_Name = c("model_with_full", "model1", "surbey_model"), 
            AIC = c(7446.8, 7206.4,7202.8))
kable(aic_summary, align = "c", format = "markdown", col.names = c("Model Name", "AIC"),
      caption = "Summary - AIC Score of Each Potential Model")
```

By summarizing the three potential models in Part I, we formed an AIC table here. The AIC of`model_with_full`, which includes all the variable in the cleaned survey dataset, `nona_survey`, is equal to 7446.8, while the one of `model1` decreases to 7206.4, and the lowest AIC appears in our final logistic regression model called `survey_model`.

```{r table3}
kable(broom::tidy(survey_model), align = "c", format = "markdown", digits = 3,
      col.names = c("Term", "Estimate", "Standard Error", "Test Statistic", "P-Value"),
      caption = "Summary for the Survey Logistic Regression Model")
```

Since `survey_model` is the final model we chose for predict the proportion of potential voters for Donald Trump, we also formed a summary table with the estimated values of the intercept, $\hat{\beta_0}$ and estimated slopes $\hat{\beta_i}$ (for i = 1,...,12) as well as standard errors, test statistics and p-value. Therefore, the fitted logistic model is:
$$
log(\frac{\hat{p}}{1-\hat{p}}) = -1.529 + 0.016 X_{age} + 0.287 X_{Labor_{Yes}} + 0.422 X_{Male} -0.119 X_{Region_{NW}} + 0.294 X_{Region_{S}} - 0.081 X_{Region_{W}}
$$
$$
- 1.87 X_{Race_{B.AA.N}} - 1.167 X_{Race_{CN}} - 0.603 X_{Race_{JP}} - 0.459 X_{Race_{O.Asian}} - 0.604 X_{Race_{Other}} + 0.14 X_{Race_{White}}.
$$

&nbsp;

```{r fig1, fig.cap = "Relationship between Voters' Choice and Number of Voters by Sex, Employment Status, Census Region and Race"}
hist_sex <- ggplot(nona_survey, aes(as.character(vote_trump), fill = sex)) + 
  geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Sex") +
  theme(plot.title = element_text(hjust = 0))

hist_lab <- ggplot(nona_survey, aes(as.character(vote_trump), fill = labforce)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People",
       fill = "Employment Status")

hist_region <- ggplot(nona_survey, aes(as.character(vote_trump), fill = census_region)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Census Region")

hist_race <- ggplot(nona_survey, aes(as.character(vote_trump), fill = race)) + geom_histogram(stat = "count") +
  theme_bw() +
  labs(x = "If vote for Donald Trump", y = "Number of People", 
       fill = "Race")

grid.arrange(hist_sex, hist_lab, hist_region, hist_race, nrow = 2, widths = c(1.55, 2), newpage = FALSE)
```

Four histograms above illustrate the relationships between the voters' choice  and number of voters in the 2020 election, grouped by some of the independent variables in the logistic `survey_model`, `sex`, `labforce`, `census_region` and `race`. 

&nbsp;

```{r fig2, fig.cap = "Trump's Potential Voters, Depending on Sex and Age"}
jitter <- nona_survey %>%
  ggplot(aes(as.character(vote_trump), age)) +  
  facet_grid(.~sex) +
  geom_jitter(aes(color = age),alpha = 0.4) +
  theme_bw() +
  labs(x = "Whether vote for Donald Trump", y = "Age") +
  theme(plot.margin = unit(c(1,1,1,1),"cm"))

grid.arrange(jitter, newpage = FALSE)
```

After grouping the data points by sex, the above scatter plot enables us to observe how both of `sex` and `age` of the voters affect the choice they make towards the 2020 election in the United States.

# Discussion

## Summary and Conclusion
The objective of this study is to demonstrate how likely Donald Trump is going to win the 2020 American election. The datasets that we used in the study are surveys and census data. The dataset of the survey that we used for predicting and modeling is retrieved from ‘Democracy Fund + UCLA Nationscape Full Data Set’. Additionally, the Census Data is based on the 2018 5-years American Community Surveys. The first step is to use the statistical method, which is Akaike's Information Criteria(AIC) in order to narrow down the variables for the logistic regression of the model. After doing AIC for our dataset, age, labor force, gender, census region, and races are the variables that we keep for our final logistic regression model. After that, we also made a prediction on the proportion of how likely voters voting for Donald Trump in the 2020 American election by using post-stratification analysis.

According to our final result, we predict that 41 % of the voters will vote for Donald Trump, who is the representative of the Republican Party. There are less than half of the people voting for him, and therefore, we could predict that he would not be elected as a president in the 2020 election based on our dataset. This prediction is related to some facts from figure \@ref(fig:fig1) and \@ref(fig:fig2). We can see that a large proportion of women do not want to vote for Donald Trump, especially for ages between 20 to 30. We could relate to his sexism, disrespected behavior, and language toward women from his past speeches. Also, we observe from the data and our predictions that the odds ratio is 1.52, which means the odds for males are 52% higher than the odds of females voting for Donald Trump. In addition, race seems to also have a significant impact on whether the persons voted for Donald Trump. According to the graph for races, there is a large proportion of black and a few races that do not want to vote for Trump. This is probably related to his racist wordings and reactions towards other races. The recent case of “George Floyd’s death” also led to the black's hatred of Donald Trump, which raises the discrimination issues of disproportioned deaths on African-Americans. The reflection on the odds ratio for the black, who vote for Donald Trump is less than 1. This implicates the blacks are less likely to vote for Donald Trump as their president. Hence, the predicted proportion of how likely Donald Trump would be voted as a president, which is 41%,  can be highly related to his inappropriate attitudes and behaviors in the past or even the rise of racism and gender inequality in America.

## Weaknesses
Logistic regression acts as a linear function, and we assume its variables to be linear. In such cases, logistic regression is a convenient tool for analyzing the `survey_data` (AmiyaRanjanRout, 2020). Nevertheless, this also means that there would be a limitation on the relationship between our independent and dependent variables. The variables must have a linear relationship. Therefore, logistic regression is not a good choice if we have nonlinear relationships between the variables that we would like to observe since the ‘surface decision (Donges, 2018)’ is linear. Additionally, logistic regression can only be used for categorical variables. The reason is that the distribution of the data is discrete, and it heavily relies on the exact independent variable we want to predict (Donges, 2018). On the other hand, overfitting is another problematic issue for logistic regression (AmiyaRanjanRout, 2020). This issue might occur when there are more features than the actual observations in the dataset. Hence, logistic regression can only deal with variables that have a linear relationship, and we have to avoid the multicollinearity of the variables. In our observations, we did not check for the linearity of the relationship between variables, but most of the data are depending on the categorical variables that we selected. Therefore, there might be a few small issues in our predictions. Also, the variables that we can predict are limited since we want the variables to be categorical. In addition, there might be some bias of the result from the census survey. The reason is that we only used the survey from the 2018 with 5-year ACS, and we decided not to predict the data from surveys from other years.

## Next Steps
With the discussion on the weakness of our prediction, there are two main issues that we could improve when we predict data in the future. The first one is that the linearity of relationship between variables is not being checked in our prediction. What we can do is to apply some model checks to see the relationships between variables, for instance, we can plot a scatter plot to see whether there is a pattern or not of the variables. The second issue can be resolved by making a few more predictions from the survey in other years. Also, we do not have to consider the year before 2012 since those years are not statistically significant. The reason is that the election can be affected by  many factors. For example, the policies that Donald Trump made for the current issue with the pandemic of COVID-19. Also, managerial and admin staff on electoral vote, industry and type of work, and income (Hyndman & Cook, 2019) would be some factors that influence the election.


# References

## Survey Data and Census Data
Steven Ruggles, Sarah Flood, Ronald Goeken, Josiah Grover, Erin Meyer, Jose Pacas and Matthew Sobek. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IPUMS USA: Version 10.0 [dataset]. Minneapolis, MN: IPUMS, 2020. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://doi.org/10.18128/D010.V10.0

Tausanovitch, Chris and Lynn Vavreck. 2020. Democracy Fund + UCLA Nationscape, October 10-17, 2019

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (version 20200814). Retrieved from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://www.voterstudygroup.org/downloads?key=44f0708b-e919-44bb-9695-2846c3531bd5.

## Other References
Auguie,B (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics. R package version 2.3.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://CRAN.R-project.org/package=gridExtra

AmiyaRanjanRout. (2020). Advantages and Disadvantages of Logistic Regression. Retrieved November 02,

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2020, from https://www.geeksforgeeks.org/advantages-and-disadvantages-of-logistic-regression/

Chen,H.W., Hu,X. & Yang,Z.C. (n.d.). Model Selection for Linear Regression Model. Retrieved from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  https://jbhender.github.io/Stats506/F17/Projects/Group21_Model_Selection.html

Donges,N. (2018). The Logistic Regression: Algorithm. Retrieved from

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://machinelearning-blog.com/2018/04/23/logistic-regression-101/

Hyndman,R.J. & Cook,D. (2019). You are what you vote: The social and demographic factors that influence

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; your vote. Retrieved from https://theconversation.com/you-are-what-you-vote-the-social-

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and-demographic-factors-that-influence-your-vote-116591


Lopez,G. (2016). Donald Trump's long history of racism, from the 1970s to 2020. Retrieved from: 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://www.vox.com/2016/7/25/12270880/donald-trump-racist-racism-history.

Lumley,T. (n.d.). postStratify. Retrieved from https://www.rdocumentation.org/packages/survey/

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; versions/2.4/topics/postStratify

Reston,M. (2020). With a shocking invocation of George Floyd, Trump shows his disconnect from 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; nation's pain. Retrieved from https://www.cnn.com/2020/06/06/politics/trump-george-floyd-maine/

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; index.html.

Wickham,H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.

Wickham,H., François,R., Henry,L. & Müller, K. (2020). dplyr: A Grammar of Data 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Manipulation. R package version 1.0.2. https://CRAN.R-project.org/package=dplyr

Wolffe,R. (2016). Donald Trump's woman problem: they don't like him, not one little bit | Richard 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wolffe. Retrieved from: https://www.theguardian.com/commentisfree/2016/jun/03/donald-trump-

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; woman-problem-female-voter-support-poll-numbers.

Wickham,H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686, 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; https://doi.org/10.21105/joss.01686
  
Xie,Y. (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Chapman and Hall/CRC. ISBN 978-1466561595
  
Xie,Y. (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ISBN 978-1498716963

Xie,Y. (2020). knitr: A General-Purpose Package for Dynamic Report Generation in R. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R package version 1.30.
